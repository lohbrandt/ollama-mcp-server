---
globs: ["**/*.py"]
description: "Testing standards and patterns for Ollama MCP Server"
tags: ["manual"]
---

# Testing Standards & Patterns

You MUST follow these testing standards for the Ollama MCP Server:

## Testing Framework Configuration

**Pytest Configuration:**
- Use `pytest` with `pytest-asyncio` for async testing
- Include coverage reporting with `pytest-cov`
- Use appropriate test markers for different test types
- Configure test discovery patterns

**Test Configuration Example:**
```python
# pytest.ini or pyproject.toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"
addopts = [
    "--verbose",
    "--tb=short",
    "--cov=src/ollama_mcp",
    "--cov-report=html",
    "--cov-report=term-missing"
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "slow: Slow tests that may be skipped",
    "ollama_required: Tests requiring Ollama server"
]
```

## Test Organization

**Directory Structure:**
```
tests/
├── unit/
│   ├── test_client.py
│   ├── test_tools/
│   │   ├── test_base_tools.py
│   │   └── test_advanced_tools.py
│   └── test_utils.py
├── integration/
│   ├── test_ollama_integration.py
│   └── test_mcp_server.py
├── fixtures/
│   ├── mock_responses.py
│   └── test_data.py
└── conftest.py
```

**Test Categorization:**
- **Unit Tests**: Test individual functions and classes in isolation
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete workflows
- **Performance Tests**: Test resource usage and timing

## Unit Testing Patterns

**Function Testing:**
- Test all code paths and edge cases
- Use mocking for external dependencies
- Test error conditions explicitly
- Verify return values and side effects

**Unit Test Example:**
```python
import pytest
from unittest.mock import AsyncMock, patch
from src.ollama_mcp.client import OllamaClient
from src.ollama_mcp.tools.base_tools import handle_base_tool

class TestBaseTools:
    @pytest.mark.asyncio
    async def test_list_models_success(self):
        """Test successful model listing."""
        # Arrange
        mock_client = AsyncMock()
        mock_client.list_models.return_value = {
            "success": True,
            "models": [
                {"name": "llama3.2", "size": "2GB", "modified": "2024-01-01"}
            ],
            "count": 1
        }
        
        # Act
        result = await handle_base_tool("list_local_models", {}, mock_client)
        
        # Assert
        assert len(result) == 1
        response = json.loads(result[0].text)
        assert response["success"] is True
        assert len(response["models"]) == 1
        assert response["models"][0]["name"] == "llama3.2"
    
    @pytest.mark.asyncio
    async def test_list_models_empty(self):
        """Test model listing with no models."""
        # Arrange
        mock_client = AsyncMock()
        mock_client.list_models.return_value = {
            "success": True,
            "models": [],
            "count": 0
        }
        
        # Act
        result = await handle_base_tool("list_local_models", {}, mock_client)
        
        # Assert
        response = json.loads(result[0].text)
        assert response["success"] is True
        assert response["total_count"] == 0
        assert "next_steps" in response
    
    @pytest.mark.asyncio
    async def test_list_models_connection_error(self):
        """Test model listing with connection error."""
        # Arrange
        mock_client = AsyncMock()
        mock_client.list_models.return_value = {
            "success": False,
            "error": "Connection refused"
        }
        
        # Act
        result = await handle_base_tool("list_local_models", {}, mock_client)
        
        # Assert
        response = json.loads(result[0].text)
        assert response["success"] is False
        assert "troubleshooting" in response
```

## Async Testing Patterns

**Async Test Standards:**
- Use `@pytest.mark.asyncio` for async test functions
- Mock async dependencies properly
- Test timeout scenarios
- Handle async context managers correctly

**Async Test Examples:**
```python
@pytest.mark.asyncio
async def test_async_operation():
    """Test async operation with proper mocking."""
    with patch('httpx.AsyncClient') as mock_client_class:
        mock_client = AsyncMock()
        mock_client_class.return_value.__aenter__.return_value = mock_client
        
        # Mock HTTP response
        mock_response = AsyncMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"status": "ok"}
        mock_client.get.return_value = mock_response
        
        # Test the async operation
        client = OllamaClient()
        result = await client.health_check()
        
        assert result["healthy"] is True
        mock_client.get.assert_called_once()

@pytest.mark.asyncio
async def test_timeout_handling():
    """Test timeout error handling."""
    with patch('httpx.AsyncClient') as mock_client_class:
        mock_client = AsyncMock()
        mock_client_class.return_value.__aenter__.return_value = mock_client
        
        # Mock timeout error
        mock_client.get.side_effect = asyncio.TimeoutError("Request timeout")
        
        client = OllamaClient()
        result = await client.health_check()
        
        assert result["healthy"] is False
        assert "timeout" in result["error"].lower()
```

## Integration Testing

**Integration Test Patterns:**
- Test real component interactions
- Use test containers or mock servers when needed
- Test cross-platform compatibility
- Include configuration variations

**Integration Test Example:**
```python
@pytest.mark.integration
@pytest.mark.ollama_required
class TestOllamaIntegration:
    @pytest.fixture(autouse=True)
    async def setup(self):
        """Setup for integration tests."""
        # Check if Ollama is available
        client = OllamaClient()
        health = await client.health_check()
        if not health["healthy"]:
            pytest.skip("Ollama server not available")
        
        self.client = client
    
    @pytest.mark.asyncio
    async def test_real_model_listing(self):
        """Test model listing against real Ollama server."""
        result = await self.client.list_models()
        
        assert result["success"] is True
        assert "models" in result
        assert "count" in result
        assert result["count"] >= 0
    
    @pytest.mark.asyncio
    async def test_real_health_check(self):
        """Test health check against real Ollama server."""
        result = await self.client.health_check()
        
        assert result["healthy"] is True
        assert result["host"] == "http://localhost:11434"
        assert "models_count" in result
```

## Mock Data and Fixtures

**Test Data Management:**
- Use pytest fixtures for reusable test data
- Create realistic mock responses
- Organize test data by scenario
- Use factory patterns for dynamic data

**Fixture Examples:**
```python
# conftest.py
@pytest.fixture
def mock_ollama_models():
    """Mock Ollama models data."""
    return [
        {
            "name": "llama3.2:latest",
            "size": 2048000000,
            "modified_at": "2024-01-01T12:00:00Z",
            "digest": "sha256:abcd1234"
        },
        {
            "name": "qwen2.5-coder:7b",
            "size": 4096000000,
            "modified_at": "2024-01-02T12:00:00Z",
            "digest": "sha256:efgh5678"
        }
    ]

@pytest.fixture
def mock_ollama_client():
    """Mock Ollama client for testing."""
    client = AsyncMock()
    client.host = "http://localhost:11434"
    client.timeout = 10.0
    return client

@pytest.fixture
def mock_health_response():
    """Mock health check response."""
    return {
        "healthy": True,
        "host": "http://localhost:11434",
        "models_count": 2,
        "message": "Ollama server is running"
    }
```

## Error Testing

**Error Scenario Coverage:**
- Test all exception types
- Test error message quality
- Test troubleshooting information
- Test graceful degradation

**Error Test Examples:**
```python
@pytest.mark.asyncio
async def test_connection_error_handling():
    """Test connection error scenarios."""
    with patch('httpx.AsyncClient') as mock_client_class:
        mock_client = AsyncMock()
        mock_client_class.return_value.__aenter__.return_value = mock_client
        
        # Test different connection errors
        error_scenarios = [
            (ConnectionError("Connection refused"), "connection_refused"),
            (asyncio.TimeoutError("Timeout"), "timeout"),
            (httpx.HTTPStatusError("404 Not Found", request=None, response=None), "not_found")
        ]
        
        client = OllamaClient()
        
        for error, expected_type in error_scenarios:
            mock_client.get.side_effect = error
            result = await client.health_check()
            
            assert result["healthy"] is False
            assert expected_type in result["error"].lower()
            assert "troubleshooting" in result

@pytest.mark.asyncio
async def test_parameter_validation_errors():
    """Test parameter validation error handling."""
    mock_client = AsyncMock()
    
    # Test missing required parameter
    result = await handle_base_tool("local_llm_chat", {}, mock_client)
    response = json.loads(result[0].text)
    assert response["success"] is False
    assert "message" in response["error"]
    
    # Test invalid parameter type
    result = await handle_base_tool("local_llm_chat", {"message": 123}, mock_client)
    response = json.loads(result[0].text)
    assert response["success"] is False
```

## Performance Testing

**Performance Test Patterns:**
- Test response times for critical operations
- Test memory usage for large operations
- Test concurrent request handling
- Test resource cleanup

**Performance Test Example:**
```python
@pytest.mark.slow
@pytest.mark.asyncio
async def test_concurrent_requests():
    """Test concurrent request handling."""
    mock_client = AsyncMock()
    mock_client.list_models.return_value = {
        "success": True,
        "models": [],
        "count": 0
    }
    
    # Test concurrent tool calls
    tasks = []
    for i in range(10):
        task = handle_base_tool("list_local_models", {}, mock_client)
        tasks.append(task)
    
    start_time = time.time()
    results = await asyncio.gather(*tasks)
    end_time = time.time()
    
    # Verify all requests completed
    assert len(results) == 10
    for result in results:
        response = json.loads(result[0].text)
        assert response["success"] is True
    
    # Verify reasonable performance
    assert end_time - start_time < 1.0  # Should complete within 1 second

@pytest.mark.asyncio
async def test_memory_usage():
    """Test memory usage for large operations."""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss
    
    # Perform memory-intensive operation
    mock_client = AsyncMock()
    large_data = ["test"] * 10000
    
    # Process large dataset
    result = await process_large_dataset(large_data, mock_client)
    
    final_memory = process.memory_info().rss
    memory_increase = final_memory - initial_memory
    
    # Verify memory usage is reasonable (less than 100MB increase)
    assert memory_increase < 100 * 1024 * 1024
```

## Test Utilities

**Test Helper Functions:**
- Create reusable test utilities
- Implement common assertion patterns
- Provide test data generators
- Include debugging helpers

**Test Utility Examples:**
```python
# tests/utils.py
def assert_successful_response(response_text: str, expected_data_keys: List[str] = None):
    """Assert that response is successful and contains expected data."""
    response = json.loads(response_text)
    assert response["success"] is True
    assert "data" in response
    
    if expected_data_keys:
        for key in expected_data_keys:
            assert key in response["data"]

def assert_error_response(response_text: str, expected_error_text: str = None):
    """Assert that response contains error with troubleshooting."""
    response = json.loads(response_text)
    assert response["success"] is False
    assert "error" in response
    
    if expected_error_text:
        assert expected_error_text in response["error"]
    
    # Should include troubleshooting information
    assert "troubleshooting" in response or "next_steps" in response

def create_mock_model_response(name: str, size_gb: float = 2.0) -> Dict[str, Any]:
    """Create mock model response data."""
    return {
        "name": name,
        "size": int(size_gb * 1024 * 1024 * 1024),
        "size_human": f"{size_gb}GB",
        "modified": "2024-01-01T12:00:00Z",
        "digest": f"sha256:{'a' * 64}"
    }
```

## Test Execution

**Running Tests:**
```bash
# Run all tests
pytest

# Run specific test categories
pytest -m unit
pytest -m integration
pytest -m "not slow"

# Run with coverage
pytest --cov=src/ollama_mcp --cov-report=html

# Run specific test file
pytest tests/unit/test_client.py

# Run with verbose output
pytest -v -s
```

**CI/CD Integration:**
- Include tests in GitHub Actions
- Test on multiple Python versions
- Test on multiple platforms
- Include coverage reporting

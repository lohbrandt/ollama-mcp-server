# Ollama MCP Server

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)

A self-contained **Model Context Protocol (MCP) server** for comprehensive Ollama management. Zero external dependencies, enterprise-grade error handling, and complete cross-platform compatibility.

## üéØ Key Features

### üîß **Self-Contained Architecture**
- **Zero External Dependencies**: No external MCP servers required
- **MIT License Ready**: All code internally developed and properly licensed
- **Enterprise-Grade**: Professional error handling with actionable troubleshooting

### üåê **Universal Compatibility**
- **Cross-Platform**: Windows, Linux, macOS with automatic platform detection
- **Multi-GPU Support**: NVIDIA, AMD, Intel detection with vendor-specific optimizations
- **Smart Installation Discovery**: Automatic Ollama detection across platforms

### ‚ö° **Complete Ollama Management**
- **Model Operations**: Download, remove, list models with progress tracking
- **Server Control**: Start, stop, monitor Ollama server with intelligent process management
- **Direct Chat**: Local model communication with automatic model selection
- **System Analysis**: Hardware compatibility assessment and resource monitoring

## üöÄ Quick Start

### Installation

```bash
git clone https://github.com/paolodalprato/ollama-mcp-server.git
cd ollama-mcp-server
pip install -e .
```

### Configuration

Add to your MCP client configuration (e.g., Claude Desktop `config.json`):

```json
{
  "mcpServers": {
    "ollama-mcp": {
      "command": "python",
      "args": [
        "D:\\MCP_SERVER\\INSTALLED\\ollama-mcp-server\\src\\ollama_mcp\\server.py"
      ],
      "env": {}
    }
  }
}
```

**Note**: Adjust the path to match your installation directory. On Linux/macOS, use forward slashes: `/path/to/ollama-mcp-server/src/ollama_mcp/server.py`

### Requirements

- **Python 3.8+**
- **Ollama installed** and accessible in PATH
- **MCP-compatible client** (Claude Desktop, etc.)

## üõ†Ô∏è Available Tools

### **Model Management**
- `list_local_models` - List installed models with details
- `local_llm_chat` - Chat directly with local models
- `download_model` - Download models with progress tracking
- `remove_model` - Safely remove models from storage

### **Server Operations**
- `start_ollama_server` - Start Ollama server (self-contained implementation)
- `ollama_health_check` - Comprehensive server health diagnostics
- `system_resource_check` - Hardware compatibility analysis

### **Advanced Features**
- `suggest_models` - AI-powered model recommendations based on your needs
- `search_available_models` - Search Ollama Hub by category
- `check_download_progress` - Monitor download progress with visual indicators
- `select_chat_model` - Interactive model selection interface

## üí¨ How to Interact with Ollama-MCP

Ollama-MCP works **through your MCP client** (like Claude Desktop) - you don't interact with it directly. Instead, you communicate with your MCP client using **natural language**, and the client translates your requests into tool calls.

### **Basic Interaction Pattern**

You speak to your MCP client in natural language, and it automatically uses the appropriate ollama-mcp tools:

```
You: "List my installed Ollama models"
‚Üí Client calls: list_local_models
‚Üí You get: Formatted list of your models

You: "Chat with llama3.2: explain machine learning"  
‚Üí Client calls: local_llm_chat with model="llama3.2" and message="explain machine learning"
‚Üí You get: AI response from your local model

You: "Check if Ollama is running"
‚Üí Client calls: ollama_health_check  
‚Üí You get: Server status and troubleshooting if needed
```

### **Example Interactions**

#### **Model Management**
- *"What models do I have installed?"* ‚Üí `list_local_models`
- *"Download qwen2.5-coder for coding tasks"* ‚Üí `download_model`
- *"Remove the old mistral model to save space"* ‚Üí `remove_model`
- *"Show me coding-focused models I can download"* ‚Üí `search_available_models`

#### **System Operations**
- *"Start Ollama server"* ‚Üí `start_ollama_server`
- *"Is my system capable of running large AI models?"* ‚Üí `system_resource_check`
- *"Recommend a model for creative writing"* ‚Üí `suggest_models`

#### **AI Chat**
- *"Chat with llama3.2: write a Python function to sort a list"* ‚Üí `local_llm_chat`
- *"Use deepseek-coder to debug this code: [code snippet]"* ‚Üí `local_llm_chat`
- *"Ask phi3.5 to explain quantum computing simply"* ‚Üí `local_llm_chat`

### **Key Points**

- **No Direct Commands**: You never call `ollama_health_check()` directly
- **Natural Language**: Speak normally to your MCP client
- **Automatic Tool Selection**: The client chooses the right tool based on your request
- **Conversational**: You can ask follow-up questions and the client maintains context

## üéØ Real-World Use Cases

### **Daily Development Workflow**
*"I need to set up local AI for coding. Check my system, recommend a good coding model, download it, and test it with a simple Python question."*

This would automatically trigger:
1. `system_resource_check` - Verify hardware capability
2. `suggest_models` - Recommend coding-focused models
3. `download_model` - Download the recommended model
4. `local_llm_chat` - Test with your Python question

### **Model Management Session**
*"Show me what models I have, see what new coding models are available, and clean up any old models I don't need."*

Triggers:
1. `list_local_models` - Current inventory
2. `search_available_models` - Browse new options
3. `remove_model` - Cleanup unwanted models

### **Troubleshooting Session**
*"Ollama isn't working. Check what's wrong, try to fix it, and test with a simple chat."*

Triggers:
1. `ollama_health_check` - Diagnose issues
2. `start_ollama_server` - Attempt to start server
3. `local_llm_chat` - Verify working with test message

### **Research and Development**
*"I'm working on a machine learning project. Suggest models good for code analysis, download the best one, and help me understand transformer architectures."*

Combines multiple tools for a complete workflow from model selection to technical learning.

## üí° Usage Examples

### Basic Operations
```
"List my installed Ollama models"
"Chat with llama3.2: explain quantum computing"
"Check Ollama server health"
```

### System Management  
```
"Start Ollama server"
"Analyze my system for AI compatibility"
"Suggest models for code generation"
```

### Model Management
```
"Download qwen2.5-coder:7b"
"Remove old models to free space"
"Search for coding-focused models"
```

## üèóÔ∏è Architecture

### **Design Principles**
- **Self-Contained**: Zero external MCP server dependencies
- **Fail-Safe**: Comprehensive error handling with actionable guidance
- **Cross-Platform First**: Universal Windows/Linux/macOS compatibility
- **Enterprise Ready**: Professional-grade implementation and documentation

### **Technical Highlights**
- **Internal Process Management**: Advanced subprocess handling with timeout control
- **Multi-GPU Detection**: Platform-specific GPU identification without confusing metrics
- **Intelligent Model Selection**: Automatic fallback and recommendation systems
- **Progressive Health Monitoring**: Smart server startup detection with detailed feedback

## üìã System Compatibility

### **Operating Systems**
- **Windows**: Full support with auto-detection in Program Files and AppData
- **Linux**: XDG configuration support with package manager integration
- **macOS**: Homebrew detection with Apple Silicon GPU support

### **GPU Support**
- **NVIDIA**: Full detection via nvidia-smi with memory and utilization info
- **AMD**: ROCm support via vendor-specific tools
- **Intel**: Basic detection via system tools
- **Apple Silicon**: M1/M2/M3 detection with unified memory handling

### **Hardware Requirements**
- **Minimum**: 4GB RAM, 2GB free disk space
- **Recommended**: 8GB+ RAM, 10GB+ free disk space
- **GPU**: Optional but recommended for model acceleration

## üîß Development

### **Project Structure**
```
ollama-mcp-server/
‚îú‚îÄ‚îÄ src/ollama_mcp/
‚îÇ   ‚îú‚îÄ‚îÄ server.py              # Main MCP server implementation
‚îÇ   ‚îú‚îÄ‚îÄ client.py              # Ollama client interface
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_tools.py      # Essential 4 tools
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ advanced_tools.py  # Extended 7 tools
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ model_manager.py       # Model operations
‚îÇ   ‚îú‚îÄ‚îÄ job_manager.py         # Background task management
‚îÇ   ‚îî‚îÄ‚îÄ hardware_checker.py    # System analysis
‚îú‚îÄ‚îÄ tests/                     # Test suite
‚îú‚îÄ‚îÄ docs/                      # Documentation
‚îî‚îÄ‚îÄ pyproject.toml            # Project configuration
```

### **Key Technical Achievements**

#### **Self-Contained Implementation**
- **Challenge**: Eliminated external `desktop-commander` dependency
- **Solution**: Internal process management with advanced subprocess handling
- **Result**: Zero external MCP dependencies, MIT license compatible

#### **Intelligent GPU Detection**
- **Challenge**: Complex VRAM reporting causing user confusion
- **Solution**: Simplified to GPU name display only
- **Result**: Clean, reliable hardware identification

#### **Enterprise Error Handling**
- **Implementation**: 6-level exception framework with specific error types
- **Coverage**: Platform-specific errors, process failures, network issues
- **UX**: Actionable troubleshooting steps for every error scenario

## ü§ù Contributing

We welcome contributions! Areas where help is especially appreciated:

- **Platform Testing**: Different OS and hardware configurations
- **GPU Vendor Support**: Additional vendor-specific detection
- **Performance Optimization**: Startup time and resource usage improvements
- **Documentation**: Usage examples and integration guides
- **Testing**: Edge cases and error condition validation

### **Development Setup**
```bash
git clone https://github.com/paolodalprato/ollama-mcp-server.git
cd ollama-mcp-server

# Install development dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Code formatting
black src/
isort src/

# Type checking
mypy src/
```

### **Testing Guidelines**
- Write tests for new features and bug fixes
- Ensure cross-platform compatibility
- Include both unit tests and integration tests
- Test error conditions and edge cases

## üìö Documentation

### **User Documentation**
- [Installation Guide](docs/installation.md)
- [Configuration Reference](docs/configuration.md)
- [Tool Reference](docs/tools.md)
- [Troubleshooting](docs/troubleshooting.md)

### **Developer Documentation**
- [Architecture Overview](docs/architecture.md)
- [API Reference](docs/api.md)
- [Contributing Guidelines](CONTRIBUTING.md)
- [Release Process](docs/release.md)

## üêõ Troubleshooting

### **Common Issues**

#### **Ollama Not Found**
```bash
# Verify Ollama installation
ollama --version

# Check PATH configuration
which ollama  # Linux/macOS
where ollama  # Windows
```

#### **Server Startup Failures**
```bash
# Check port availability
netstat -an | grep 11434

# Manual server start for debugging
ollama serve
```

#### **Permission Issues**
- **Windows**: Run as Administrator if needed
- **Linux/macOS**: Check user permissions for service management

### **Getting Help**
- **Issues**: [GitHub Issues](https://github.com/paolodalprato/ollama-mcp-server/issues)
- **Discussions**: [GitHub Discussions](https://github.com/paolodalprato/ollama-mcp-server/discussions)
- **Documentation**: Check the `docs/` directory

## üìä Performance

### **Typical Response Times**
- **Health Check**: <500ms
- **Model List**: <1 second
- **Server Start**: 1-15 seconds (hardware dependent)
- **Model Chat**: 2-30 seconds (model and prompt dependent)

### **Resource Usage**
- **Memory**: <50MB for MCP server process
- **CPU**: Minimal when idle, scales with operations
- **Storage**: Configuration files and logs only

## üîê Security

- **Local Processing**: All AI operations performed locally
- **No Data Transmission**: No external API calls or data sharing
- **Process Isolation**: Secure subprocess management
- **Configuration Security**: Safe handling of configuration files

## üìà Roadmap

### **Version 1.0 Goals**
- [ ] Comprehensive test suite with CI/CD
- [ ] Performance optimizations and benchmarking
- [ ] Additional GPU vendor support (AMD ROCm, Intel Arc)
- [ ] Configuration management UI
- [ ] Docker containerization support

### **Future Enhancements**
- [ ] Model performance analytics
- [ ] Distributed model management
- [ ] Advanced scheduling and queuing
- [ ] Integration with popular AI frameworks
- [ ] Web dashboard interface

## üë®‚Äçüíª About This Project

This is my first MCP server, born from practical necessity and real-world usage. As someone who works daily with AI tools and helps companies integrate generative AI into their workflows, I found myself constantly switching between different interfaces to manage local Ollama models.

### **The Problem I Faced**
- Switching between command line and GUI tools for model management
- Inconsistent model information across different interfaces  
- Complex server startup procedures when Ollama wasn't running
- Difficulty recommending appropriate models to clients and students

### **My Solution**
I built this MCP server to streamline my own workflow, and then refined it into a production-ready tool that others might find useful. The design reflects real usage patterns:

- **Self-contained**: No external dependencies that can break
- **Intelligent error handling**: Clear guidance when things go wrong
- **Cross-platform**: Works consistently across different environments  
- **Practical tools**: Features I actually use in daily work

### **Design Philosophy**
This isn't just another technical project - it's a tool built by someone who uses it daily. Every feature addresses a real problem I encountered while working with local AI models, training clients, or helping companies evaluate AI integration options.

The extensive documentation and error handling reflect my experience supporting diverse users with varying technical backgrounds, from developers to business users taking their first steps with local AI.

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **Ollama Team**: For the excellent local AI platform
- **MCP Project**: For the Model Context Protocol specification
- **Claude Desktop**: For MCP client implementation and testing platform
- **Community**: For testing, feedback, and contributions

## üìû Support

- **Bug Reports**: [GitHub Issues](https://github.com/paolodalprato/ollama-mcp-server/issues)
- **Feature Requests**: [GitHub Issues](https://github.com/paolodalprato/ollama-mcp-server/issues)
- **Community Discussion**: [GitHub Discussions](https://github.com/paolodalprato/ollama-mcp-server/discussions)
- **Documentation**: [docs/](docs/) directory

---

**Status**: Production Ready - v0.9 FINAL  
**License**: MIT  
**Platform**: Cross-Platform (Windows/Linux/macOS)  
**Dependencies**: Zero external MCP servers required